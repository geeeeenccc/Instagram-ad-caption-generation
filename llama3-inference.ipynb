{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8498461,"sourceType":"datasetVersion","datasetId":5071317}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qqq transformers --progress-bar off\n!pip install -qqq bitsandbytes --progress-bar off  # If you used 4-bit quantization\n!pip install -qqq peft torch --progress-bar off\n!pip install -qqq gradio --progress-bar off","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-24T11:41:45.523014Z","iopub.execute_input":"2024-05-24T11:41:45.523366Z","iopub.status.idle":"2024-05-24T11:42:49.354812Z","shell.execute_reply.started":"2024-05-24T11:41:45.523335Z","shell.execute_reply":"2024-05-24T11:42:49.353719Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nspacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nweasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel, PeftConfig\nfrom huggingface_hub import login","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:42:49.357121Z","iopub.execute_input":"2024-05-24T11:42:49.357836Z","iopub.status.idle":"2024-05-24T11:42:56.610695Z","shell.execute_reply.started":"2024-05-24T11:42:49.357796Z","shell.execute_reply":"2024-05-24T11:42:56.609924Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:42:56.611841Z","iopub.execute_input":"2024-05-24T11:42:56.612351Z","iopub.status.idle":"2024-05-24T11:42:56.619938Z","shell.execute_reply.started":"2024-05-24T11:42:56.612318Z","shell.execute_reply":"2024-05-24T11:42:56.619053Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"hf_token = \"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\"\n\nlogin(hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:42:56.622368Z","iopub.execute_input":"2024-05-24T11:42:56.622740Z","iopub.status.idle":"2024-05-24T11:42:56.883256Z","shell.execute_reply.started":"2024-05-24T11:42:56.622707Z","shell.execute_reply":"2024-05-24T11:42:56.882222Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\nPaths to saved model and tokenizer\n\"\"\"\n\n# Model name we want to use\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n# The directory where the model and tokenizer are saved\nsource_dir = \"/kaggle/input/llama3-ig-ad-generation-task/experiments/\"\ncheckpoint_dir = source_dir + \"checkpoint-483/\"\n# source_dir = \"/kaggle/input/llama3-ig-ad-generation-task/experiments\" ","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:42:56.884417Z","iopub.execute_input":"2024-05-24T11:42:56.884748Z","iopub.status.idle":"2024-05-24T11:42:56.889914Z","shell.execute_reply.started":"2024-05-24T11:42:56.884721Z","shell.execute_reply":"2024-05-24T11:42:56.889015Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Quantize the base model\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    use_safetensors=True,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n\n# Load the PEFT configuration\npeft_config = PeftConfig.from_pretrained(source_dir)\n\n# Load the adapter\nmodel = PeftModel.from_pretrained(base_model, source_dir)\n\n# Ensure the tokenizer padding\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:42:56.891267Z","iopub.execute_input":"2024-05-24T11:42:56.891659Z","iopub.status.idle":"2024-05-24T11:48:07.623271Z","shell.execute_reply.started":"2024-05-24T11:42:56.891594Z","shell.execute_reply":"2024-05-24T11:48:07.622361Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528f3ed8eb1b42d9b6cb7bd2bddc6f9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83aeb33e5c70480e8b6ea5bc9e4d591b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d7a889c8de84be29e273ee2809c96c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"781bc42d4a9e46648a3b796cfa2f68d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"904a2f26fd3d434ebbb6af5a4f849cd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d9643cb5eef450c9e3540825115090b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9a688ce77841878a05e9c357c54a80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b006b7cf74f459ebcd19ff3f1a195ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc48d583ab5b4e0da8fb8696fed90521"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now the model and tokenizer are loaded and ready for inference","metadata":{}},{"cell_type":"code","source":"default_system_prompt = \"\"\"\nWrite an engaging Instagram post caption about the given input. You can generate a few heashtags.\n\"\"\".strip()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:48:07.624684Z","iopub.execute_input":"2024-05-24T11:48:07.625078Z","iopub.status.idle":"2024-05-24T11:48:07.630172Z","shell.execute_reply.started":"2024-05-24T11:48:07.625041Z","shell.execute_reply":"2024-05-24T11:48:07.629248Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def generate_prompt(conversation: str, system_prompt: str = default_system_prompt) -> str:\n    return f\"\"\"### Instruction: {system_prompt}\n\n### Input:\n{conversation.strip()}\n\n### Response:\n\"\"\".strip()\n\ndef clean_generated_text(text: str) -> str:\n    # Remove duplicate hashtags\n    hashtags = set()\n    cleaned_text = []\n    for word in text.split():\n        if word.startswith(\"#\"):\n            if word.lower() not in hashtags:\n                hashtags.add(word.lower())\n                cleaned_text.append(word)\n        else:\n            cleaned_text.append(word)\n            \n    # There may be to that function, \n    # but for now we'll proccess the duplicates\n    return \" \".join(cleaned_text)\n\ndef generate_post(model, text: str):\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    inputs_length = len(inputs[\"input_ids\"][0])\n    with torch.no_grad():\n        outputs = model.generate(**inputs, \n                                 max_new_tokens=100, \n                                 temperature=0.7, \n                                 top_p=0.95)\n    generated_text = tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True)\n    return clean_generated_text(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:48:07.631549Z","iopub.execute_input":"2024-05-24T11:48:07.631838Z","iopub.status.idle":"2024-05-24T11:48:07.645073Z","shell.execute_reply.started":"2024-05-24T11:48:07.631813Z","shell.execute_reply":"2024-05-24T11:48:07.644127Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Test the function with a sample instruction\nsample_instruction = \"Create a new post about the 'Adventure' model backpack with 25 liters capacity for $200, perfect for climbers.\"\nprompt = generate_prompt(sample_instruction)\ngenerated_post = generate_post(model, prompt)\nprint(\"Generated Post Content:\\n\", generated_post)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:48:07.646242Z","iopub.execute_input":"2024-05-24T11:48:07.646593Z","iopub.status.idle":"2024-05-24T11:48:32.280966Z","shell.execute_reply.started":"2024-05-24T11:48:07.646561Z","shell.execute_reply":"2024-05-24T11:48:32.280038Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n2024-05-24 11:48:10.105413: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 11:48:10.105523: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 11:48:10.232639: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Generated Post Content:\n ðŸŒ„ Who's ready to conquer new peaks? The 'Adventure' model is here for those who love to take the road less traveled - a comfortable, weather-resistant backpack with a trekking main compartment and trekking shoulder straps, ready for your next 3-day peak climb or week-long trek. With a 25-liter capacity and a 200 dollar price tag, the 'Adventure' is the perfect companion for anyone looking for a reliable daypack that has plenty of space for your gear.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Gradio interface\nimport gradio as gr\n\n\ndef gradio_interface(instruction):\n    prompt = generate_prompt(instruction)\n    return generate_post(model,prompt)\n\ninterface = gr.Interface(\n    fn=gradio_interface,\n    inputs=gr.Textbox(lines=3, placeholder=\"Enter instruction here...\"),\n    outputs=\"text\"\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:48:32.283452Z","iopub.execute_input":"2024-05-24T11:48:32.283978Z","iopub.status.idle":"2024-05-24T11:48:35.691566Z","shell.execute_reply.started":"2024-05-24T11:48:32.283950Z","shell.execute_reply":"2024-05-24T11:48:35.690732Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Launch gradio interface\ninterface.launch()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:48:35.692684Z","iopub.execute_input":"2024-05-24T11:48:35.692972Z","iopub.status.idle":"2024-05-24T11:48:40.710772Z","shell.execute_reply.started":"2024-05-24T11:48:35.692947Z","shell.execute_reply":"2024-05-24T11:48:40.709912Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\nRunning on public URL: https://61aa336793b716414d.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://61aa336793b716414d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}